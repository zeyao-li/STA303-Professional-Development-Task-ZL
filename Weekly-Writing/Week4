Okay, I can help you recalling what maximum likelihood estimation is. Let’s start by talking about a simple example. Suppose we have a box, and there is a red ball and blue ball in it. Each time, we pick a ball from the box and put it back. After three turns, we get the pattern of “red, blue, red”. Question we might ask in this case is “what is the probability of picking red balls that results in this pattern?” Well, this is when the maximum likelihood estimation comes into play. 

By taking second year statistics courses, you are probably familiar with the concept of estimator and parameter. A parameter is an unknown constant we are interested in, and its estimator is a random variable. We use the estimator to approximate to the true value of parameter. In our example, the parameter is the probability of picking red balls. In order to determine its value, we can calculate the likelihood function, which is how likely the pattern of “red, blue, red” occurs given different values of the probability of picking red balls. As the likelihood function increases, it becomes more likely to observe the pattern. In particular, the maximum likelihood estimator has the estimate that corresponding to the maximum of the likelihood function. In other words, the maximum likelihood estimate is closest to the true probability of picking red balls, and by using the estimate, we are most likely observing the pattern of “red, blue, red”. 

In more general cases, given a sample data, we can use maximum likelihood estimation to estimate the interested parameter from statistical models such as linear regression. The resulting estimates are most likely to produce data like the sample data through the model we use. 
