For any published findings, it is important to make sure the results are reproducible and truly significant. However, in the article “Common misconceptions about data analysis and statistics”(Motulsky 2014), it shows that even for experienced investigator having the proper tools, they could not reproduce a lot of published findings. One of the major reasons behind this is P-hacking.

With P-hacking, the face values of results could not be interpreted. Since any results obtained by P-hacking are intentionally created, they are not very meaningful. The procedures of many P-hackings are similar in nature. In the beginning, some sample data are collected, and they are analyzed by various statistical approaches. After the results are collected, researchers pay most attention to the p-value of the study. If the p-value is less than the benchmark significance level of 0.05, then the results would be interpreted as significant, and conclusions would be made. On the other hand, if the p-value is higher than 0.05, researchers would try to manipulate the data and analyze again even though it is clear that results are insignificant. There are many types of data manipulations, which include transformations of data, additions of new explanatory or dependent variables, and replacements of outliers. Meanwhile, an example of P-hacking is Ad hoc sample size selection. This happens as researchers do not specify the sample size before conducting study, but instead, they obtain the final result by continually trying different sample size until the result is satisfied.

Therefore, it is crucial for scientific paper to state whether P-hacking is used throughout the study. But more importantly, it is better to prevent the use of P-hacking to make sure the results are reproducible and reduce potential bias. 



Reference:

Motulsky, H. J. (2014). Common misconceptions about data analysis and statistics. Naunyn-Schmiedeberg’s Archives of Pharmacology, 387(11), 1017–1023. https://doi.org/10.1007/s00210-014-1037-6
